Diferencias entre Gradient Boosting Classifier y XGBoost Classifier. 
Gradient Boosting Classifier intenta crear un algoritmo robusto en su aprendizaje a través de pequeños algoritmos no tan fuertes. En este algoritmo se intenta abortar el problema desde un ejercicio de optimización en donde la función objetivo es el error y se busca optimizarlo. Esto funciona ajustando un algoritmo no tan fuerte y en cada paso se ensambla con otro algoritmo con condiciones similares que, combinados, ayuda a mejorar el desempeño del algoritmo en general. Se generar una iteración que en cada paso evalúa el desempeño del modelo respecto al error comparando el resultado predicho contra el resultado real y actualizado el resultado con el objetivo de minimizarlo. 
Por otra parte, XGBoost Classifier tiene algunas características que lo diferencia: Tiene penalización inteligente de los árboles, tiene mayor aleatoriedad de los parámetros y una reducción proporcional de los nodos. La aleatoriedad extra que presenta este algoritmo permite disminuir la correlación existente entre los árboles y esto hace que a menor correlación, mejor ensamblaje de los algoritmos. Adicionalmente, XGBoost Classifier es más rápido en rendimiento que Gradient Boostieng Classifier pero así mismo Gradient Boosting Classifier tiene un mayor rango de aplicaciones prácticas. 
Por otra parte, Gradient Boosting Classifier tiene una gran cantidad de parámetros para controlar el crecimiento de los árboles de decisión del algoritmo, mientras que en XGBoosting Classifier el tamaño de los árboles de decisión y la magnitud de los pesos son controlados por parámetros estandarizados y regulados. Esta regularización de los parámetros provee al algoritmo de mayor robustez para predecir los resultados. 
