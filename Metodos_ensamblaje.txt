Los modelo de machine learning pueden tener un potencial aún mayor por medio de los métodos de ensamblaje. Los métodos de ensamblaje es una técnica de machine learning que combina varios modelos con el objetivo de construir un modelo predictivo óptimo. Los métodos de ensamblaje permiten reducir la varianza (bagging), el sesgo (boosting) o mejorar las predicciones (stacking). 
Existen métodos de ensamblaje homogéneos y heterogéneos. Los homogéneos utilizan como base el mismo algoritmo de machine learning, mientras que los heterogéneos combinan diferentes algoritmos. 
La ventaja que ofrecen los métodos de ensamblaje en el campo de machine learning es que al usar múltiples modelos y combinar sus resultados, generalmente se incrementa el desempeño del modelo o al menos disminuye la probabilidad de escoger un modelo no tan bueno. En las competencias de machine learning varios ganadores han utilizado métodos de ensamblaje para mejorar la precisión de sus modelos: la competencia de Netflix, varias competencias en Kaggle y KDD.  
Hay tres tipos de ensamblaje: Bagging, Boosting y Stacking. El método Bagging realiza muchas predicciones haciendo remuestreo de la base de entrenamiento, esto permite reducir la varianza del error. El método Boosting, es un método iterativo que ajusta el peso de una observación basado en la última clasificación, si una observación es clasificada incorrectamente el modelo intenta darle mayor peso mientras que a las observaciones correctamente clasificadas les da menor peso. Y finalmente, Stacking, utiliza las salidas de diferentes modelos ensamblados para construir un nuevo modelo. 
